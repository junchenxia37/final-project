# QwenÂ BusinessÂ StrategyÂ Fineâ€‘TuningÂ &Â Evaluation

## Overview
This repository contains all the code and data you need to preprocess conversations, fineâ€‘tune **Qwen** on businessâ€‘scenario strategy prompts, and evaluate the resulting model.

1. **Preprocess** raw dialogue data into JSONL format  
2. **Fineâ€‘tune** the base Qwen model with the processed data  
3. **Run inference** to test the model on sample strategy questions  
4. **Compare** baseline and fineâ€‘tuned outputs using Tokenâ€‘F1, ROUGEâ€‘L, and semantic similarity  

---

## RepositoryÂ Structure
```text
.
â”œâ”€â”€ preprocess.ipynb         â”€â”€ Data cleaningÂ & JSONL preparation
â”œâ”€â”€ finetune.py              â”€â”€ Fineâ€‘tuning script (SFTTrainer configuration)
â”œâ”€â”€ run_finetune_slurm.sh    â”€â”€ SLURM wrapper for cluster training
â”œâ”€â”€ test_inference.py        â”€â”€ Quick inference on sample prompts
â”œâ”€â”€ compare.py               â”€â”€ Full evaluation & comparison
â””â”€â”€ requirements.txt         â”€â”€ Python package list (optional)
```

---

## Requirements
* PythonÂ â‰¥â€¯3.10  
* PyTorch + CUDA **or** HIP (for NVIDIA / AMD GPUs)  
* [ðŸ¤—Â Transformers](https://github.com/huggingface/transformers)  
* [ðŸ¤—Â Datasets](https://github.com/huggingface/datasets)  
* [TRL](https://github.com/huggingface/trl)  
* (Optional) Jupyter for running `preprocess.ipynb`  

> **Installation tip:**Â Make sure your GPU driver & CUDA/HIP toolkit match the PyTorch wheel you install.

---

## HowÂ toÂ Run

### 1Â Â Clone the repo & create a fresh environment
```bash
git clone <yourâ€‘forkâ€‘url> qwenâ€‘bizâ€‘strategy
cd qwenâ€‘bizâ€‘strategy

# Create an env (name it however you like)
conda create -n qwenâ€‘biz python=3.10 -y
conda activate qwenâ€‘biz

# Install packages
pip install -r requirements.txt          # preferred
# â”€â”€ or, manual â”€â”€
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121             transformers datasets trl jupyter
```

### 2Â Â Preâ€‘process raw dialogue
Run the notebook interactively **or** headless; it writes `data/processed_data.jsonl` by default.
```bash
jupyter notebook preprocess.ipynb                  # interactive
# OR
jupyter nbconvert --execute --to notebook --inplace preprocess.ipynb
```
Adjust `OUTPUT_PATH` inside the notebook if you need a different location.

### 3Â Â Singleâ€‘GPU fineâ€‘tuning (local)
```bash
python finetune.py   --model_name_or_path <BASE_MODEL_DIR>   --train_file data/processed_data.jsonl   --output_dir checkpoints/qwenâ€‘bizâ€‘sft   --per_device_train_batch_size 4   --gradient_accumulation_steps 8   --num_train_epochs 3
```
*If you hit OOM errors, lower `--per_device_train_batch_size` or add `--fp16`.*

### 4Â Â Multiâ€‘GPU / cluster fineâ€‘tuning (SLURM)
Make sure `run_finetune_slurm.sh` points to the same JSONL file and model path, then submit:
```bash
sbatch run_finetune_slurm.sh
```
The script contains typical SLURM directives:
```bash
#SBATCH --nodes=1
#SBATCH --gpus-per-node=4
#SBATCH --time=04:00:00
```
Adapt partition/queue, container image, and resource specs to your cluster.

### 5Â Â Quick inference sanity check
```bash
python test_inference.py   --model_dir checkpoints/qwenâ€‘bizâ€‘sft   --prompt "As the COO of a retail chain, how would youâ€¦?"
```

### 6Â Â Full evaluation & model comparison
1. **Baseline** model answers (cached once):
   ```bash
   python compare.py --model_dir <BASE_MODEL_DIR> --save_preds baseline.json
   ```
2. **Fineâ€‘tuned** model answers:
   ```bash
   python compare.py --model_dir checkpoints/qwenâ€‘bizâ€‘sft --save_preds finetune.json
   ```
3. **Metrics** comparison:
   ```bash
   python compare.py --evaluate      --baseline baseline.json      --fine_tuned finetune.json
   ```
   Results (Tokenâ€‘F1, ROUGEâ€‘L, cosine similarity) print to console and save to `results/`.

### 7Â Â Resume training after interruption
```bash
python finetune.py   --resume_from_checkpoint checkpoints/qwenâ€‘bizâ€‘sft   --continue_training
```

---

## GotchasÂ & Tips
* For very long dialogues, raise `--model_max_length` and `--block_size` inside `finetune.py` (e.g.,â€¯4096â€¯tokens).  
* On AMD ROCm, avoid fragmentation by exporting:  
  ```bash
  export PYTORCH_HIP_ALLOC_CONF=expandable_segments:True
  ```  
